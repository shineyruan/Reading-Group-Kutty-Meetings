\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{geometry,indentfirst,soul}
\usepackage{amsmath,amssymb,bm,amsthm}
\usepackage{enumitem}
\usepackage[hidelinks]{hyperref}
\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}

\newtheorem{theorem}{Theorem}[section]

\title{\textbf{Summary about Prediction Market}}
\author{Zhihao Ruan, Jiayu Yi, Naihao Deng\\\texttt{\{ruanzh,yijiayu,dnaihao\}@umich.edu}}

\begin{document}
\maketitle
\tableofcontents


\newpage
\section{Exponential Family}
Distributions over $\mathbf{x}$ with $\emph{natural parameters}$ $\bm{\eta}$ that are of the form
\begin{equation}\label{eq:expo_fam}
    p(\mathbf{x}|\bm{\eta})=h(\mathbf{x})g(\bm{\eta})\exp(\bm{\eta}^T\mathbf{u}(\mathbf{x}))
\end{equation}
We need to make sure that
$\int{p(\mathbf{x}|\bm{\eta})}d\bm{x}=1$, therefore
\begin{equation}\label{eq:normalize}
    g(\bm{\eta})\int{h(\mathbf{x})\exp\left\{\bm{\eta}^T\mathbf{u}(\mathbf{x})\right\}}d\mathbf{x}=1
\end{equation}
$g(\bm{\eta})$ is a coefficient that is needed to ensure that the distribution is normalized.
\subsection{Sufficient Statistics}
If we take the partial derivative with respect to $\bm{\eta}$ from both sides of equation \ref{eq:normalize}, we have
\begin{equation}\label{eq:derivative}
    \nabla g(\bm{\eta})\int{h(\mathbf{x})\exp\left\{\bm{\eta}^T\mathbf{u}(\mathbf{x})\right\}}d\mathbf{x} + g(\bm{\eta})\int{h(\mathbf{x})\exp\left\{\bm{\eta}^T\mathbf{u}(\mathbf{x})\right\}}\mathbf{u}(\mathbf{x})d\mathbf{x}=0
\end{equation}
If we plug in \ref{eq:normalize} to \ref{eq:derivative}, we have
\begin{equation}
    -\frac{1}{g(\bm{\eta})}\nabla g(\bm{\eta})=g(\bm{\eta})\int{h(\mathbf{x})\exp\left\{\bm{\eta}^T\mathbf{u}(\mathbf{x})\right\}}\mathbf{u}(\mathbf{x})d\mathbf{x}=\mathbb{E}[\mathbf{u}(\mathbf{x})]
\end{equation}
Since
\begin{displaymath}
    \nabla \ln{g(\bm{\eta})} = \frac{1}{g(\bm{\eta})}\nabla g(\bm{\eta})
\end{displaymath}
It can be deduced that
\begin{equation}
    -\nabla \ln{g(\bm{\eta})} = \mathbb{E}[\mathbf{u}(\mathbf{x})]
\end{equation}
If we consider a set of independent identically distributed data denoted by $\mathbf{x}=\left\{\mathbf{x}_1,\dots,\mathbf{x}_n\right\}$, the likelihood function is given by
\begin{equation}\label{eq:likelihood}
    p(\mathbf{x}|\bm{\eta})=\left(\prod_{n=1}^{N}h(\mathbf{x}_n)\right)g(\bm{\eta})^N\exp\left\{\bm{\eta}^T\sum_{n=1}^N\mathbf{u}(\mathbf{x}_n)\right\}
\end{equation}
To produce the maximum likelihood estimator $\bm{\eta}_{ML}$, we need to take the partial derivative of \ref{eq:likelihood} and set its value to 0.
\begin{equation}\label{eq:maximum_likelihood}
    \nabla \left(\left(\prod_{n=1}^{N}h(\mathbf{x}_n)\right)g(\bm{\eta})^N\exp\left\{\bm{\eta}^T\sum_{n=1}^N\mathbf{u}(\mathbf{x}_n)\right\}\right) =0
\end{equation}
By expanding \ref{eq:maximum_likelihood} it can be seen that
\begin{displaymath}
    \left(\prod_{n=1}^{N}h(\mathbf{x}_n)\right)\left(Ng(\bm{\eta})^{N-1}\nabla g(\bm{\eta})\exp\left\{\bm{\eta}^T\sum_{n=1}^N\mathbf{u}(\mathbf{x}_n)\right\}+g(\bm{\eta})^N \exp\left\{\bm{\eta}^T\sum_{n=1}^N\mathbf{u}(\mathbf{x}_n)\right\}\sum_{n=1}^N\mathbf{u}(\mathbf{x}_n)\right)=0
\end{displaymath}
Therefore,
\begin{equation}\label{eq:estimator}
    -\nabla \ln{g(\bm{\eta}_{ML})} = \frac{1}{N}\sum_{n=1}^N\mathbf{u}(\mathbf{x}_n)
\end{equation}
From equation \ref{eq:estimator} it can be seen that $\bm{\eta}_{ML}$ can be determined if $\mathbf{u}(\mathbf{x}_n)$ is known, $\mathbf{u}(\mathbf{x}_n)$ is therefore called the \textbf{sufficient statistics}. If $N\rightarrow \infty$, the right hand side of \ref{eq:estimator} would become $\mathbb{E}[\mathbf{u}(\mathbf{x})]$ and $\bm{\eta}_{ML}$ would be the true $\bm{\eta}$.
\subsection{Conjugate Priors}
Conjugate priors are priors that lead to their corresponding posterior distributions to have the same form as they do.
For exponential family distributions, their conjugate priors are of the form
\begin{equation}\label{eq:conjugate_prior}
    p(\boldsymbol{\eta} | \boldsymbol{\chi}, \nu)=f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^{\nu} \exp \left\{\nu \boldsymbol{\eta}^{\mathrm{T}} \boldsymbol{\chi}\right\}
\end{equation}
$\nu$ can be interpreted as the number of samples that are observed. $\bm{\chi}$ is the mean of the pseudo observation of the sufficient statistics $\mathbf{u}(\mathbf{x})$. Each one of the $\nu$ observations take the value of $\bm{\chi}$.
The updated posterior can be shown as
\begin{equation}\label{eq:posterior_0}
    \begin{split}
        p(\boldsymbol{\eta} | \mathbf{X}, \boldsymbol{\chi}, \nu) &=
        \left(\prod_{n=1}^{N}h(\mathbf{x}_n)\right)g(\bm{\eta})^N\exp\left\{\bm{\eta}^T\sum_{n=1}^N\mathbf{u}(\mathbf{x}_n)\right\}\times f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^{\nu} \exp \left\{\nu \boldsymbol{\eta}^{\mathrm{T}} \boldsymbol{\chi}\right\}\\
        &\propto
        g(\boldsymbol{\eta})^{\nu+N} \exp \left\{\boldsymbol{\eta}^{\mathrm{T}}\left(\sum_{n=1}^{N} \mathbf{u}\left(\mathbf{x}_{n}\right)+\nu \bm{\chi}\right)\right\}
    \end{split}
\end{equation}
It can been seen from here that we can update the posterior with $\nu \leftarrow \nu+N$ and $\nu\bm{\chi} \leftarrow \sum_{n=1}^{N} \mathbf{u}\left(\mathbf{x}_{n}\right)+\nu \bm{\chi}$. \textbf{This update rule again emphasizes that $\nu$ is the number of pseudo observations, and $\bm{\chi}$ is the mean of the sufficient statistics $\mathbf{u}(\mathbf{x})$ of the pseudo observations.}
\subsubsection{Bernoulli Distribution}
The conjugate prior for Bernoulli distribution is the Beta distribution. The probability density function for Beta distribution is
\begin{equation}\label{eq:beta}
    p(x| a,b)=\frac{x^{a-1}(1-x)^{b-1}}{\mathbf{B}(a, b)}
\end{equation}
When it is written in the form of the conjugate prior
\begin{equation}\label{eq:bernoulli}
    \begin{split}
        \frac{\theta^{a-1}(1-\theta)^{b-1}}{\mathbf{B}(a, b)}
        =\frac{1}{\mathbf{B}(a, b)}(1-\theta)^{a+b-2}\exp\left[ (a+b-2)\ln\left( \frac{\theta}{1-\theta} \right)\frac{a-1}{a+b-2} \right]
    \end{split}
\end{equation}
From equation \ref{eq:bernoulli} we can see that $\nu$ corresponds to $a+b-2$ here and $\bm{\chi}$ corresponds to $\frac{a-1}{a+b-2}$.
\subsubsection{Normal Distribution}
Given that the conjugate priors for exponential family distributions are of the form $p(\boldsymbol{\eta} | \boldsymbol{\chi}, \nu)=f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^{\nu} \exp \left\{\nu \boldsymbol{\eta}^{\mathrm{T}} \boldsymbol{\chi}\right\}$, we were trying to find the $\nu$ and $\chi$ for univariate Gaussian distribution given that the variance $\sigma^2$ is fixed.

The probability density function of univariate Gaussian distribution is given by
\begin{equation}\label{eq:gaussian}
    p(x|\mu)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
\end{equation}
First we can start by proving that the conjugate prior for Gaussian distribution is also a Gaussian distribution (when the variance is fixed).
\begin{displaymath}
    \begin{array}{l}{p(x | \mu) \sim N\left(\mu, \sigma^{2}\right)} \\ {p(\mu) \sim N\left(\mu_{0}, \sigma_{0}^{2}\right)}\end{array}
\end{displaymath}
Suppose $D$ is the set of data points that we have observed. We have
\begin{equation}\label{eq:posterior}
    \begin{aligned}
        p(\mu | \mathcal{D}) & \propto  \prod_{i=1}^{N} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{1}{2}\left(\frac{x^{(i)}-\mu}{\sigma}\right)^{2}\right\} \times \frac{1}{\sqrt{2 \pi} \sigma_{0}} \exp \left\{-\frac{1}{2}\left(\frac{\mu-\mu_{0}}{\sigma_{0}}\right)^{2}\right\} \\
                             & \propto \exp\left\{-\frac{1}{2}\left[\sum_{i=1}^{N}\left(\frac{x^{(i)}-\mu}{\sigma}\right)^{2}+\left(\frac{\mu-\mu_{0}}{\sigma_{0}}\right)^{2}\right]\right\}                                                                                               \\
                             & \propto \exp \left\{-\frac{1}{2}\left[\left(\frac{N}{\sigma^{2}}+\frac{1}{\sigma_{0}^{2}}\right) \mu^{2}-2\left(\frac{\sum_{i=1}^{N} x^{(i)}}{\sigma^{2}}+\frac{\mu_{0}}{\sigma_{0}^{2}}\right) \mu\right]\right\}
    \end{aligned}
\end{equation}
We can see that equation \ref{eq:posterior} can be easily written in the form of a Gaussian distribution if we extract multiple from the coefficient in the front.

\ul{Can all this be still written in the form of exponential family distribution and exponential family distribution's conjugate priors?}

We want to write the prior
$$\frac{1}{\sqrt{2 \pi} \sigma_{0}} \exp \left\{-\frac{1}{2}\left(\frac{\mu-\mu_{0}}{\sigma_{0}}\right)^{2}\right\}$$ in the form of $p(\boldsymbol{\eta} | \boldsymbol{\chi}, \nu)=f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^{\nu} \exp \left\{\nu \boldsymbol{\eta}^{\mathrm{T}} \boldsymbol{\chi}\right\}$, and the posterior 
\begin{displaymath}
\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2}\left(\frac{x^{(i)}-\mu}{\sigma}\right)^2\right\}
\end{displaymath} in the form of $p(\mathbf{x}|\bm{\eta})=\left(\prod_{n=1}^{N}h(\mathbf{x}_n)\right)g(\bm{\eta})^{N}\exp\left\{\bm{\eta}^T\sum_{n=1}^{N}\mathbf{u}(\mathbf{x}_n)\right\}$.

\textbf{We can first start with priors.}
\begin{equation}\label{eq:deduction}
    \begin{split}
        \frac{1}{\sqrt{2 \pi} \sigma_{0}} \exp \left\{-\frac{1}{2}\left(\frac{\mu-\mu_{0}}{\sigma_{0}}\right)^{2}\right\}&=\frac{1}{\sqrt{2 \pi} \sigma_{0}}\exp \left\{-\frac{\mu^2-2{\mu}_0\mu+{\mu_0}^2}{2{\sigma_0}^2}\right\}
    \end{split}
\end{equation}
From the equation above we can deduce that the $\boldsymbol{\eta}$ should be $
    \begin{bmatrix}
        \mu   \\
        \mu^2 \\
    \end{bmatrix}$.
\underline{Is this OK?}
Continue with Eq. \ref{eq:deduction},
\begin{equation}\label{eq:deduce}
    \begin{split}
        \frac{1}{\sqrt{2 \pi} \sigma_{0}}\exp \left\{-\frac{\mu^2-2{\mu}_0\mu+{\mu_0}^2}{2{\sigma_0}^2}\right\}&=\frac{1}{\sqrt{2 \pi} \sigma_{0}}\exp \left\{\boldsymbol{\eta}^T\begin{bmatrix}
            \frac{\mu_0}{{\sigma_0}^2} \\
            -\frac{1}{2{\sigma_0}^2}   \\
        \end{bmatrix}-\frac{{\mu_0}^2}{2{\sigma_0}^2}\right\}\\
        &=\frac{1}{\sqrt{2 \pi} \sigma_{0}}\exp{\left(\frac{-{\mu_0}^2}{2{\sigma_0}^2}\right)}\exp{\left[\boldsymbol{\eta}^T\begin{bmatrix}
                    \frac{\mu_0}{{\sigma_0}^2} \\
                    -\frac{1}{2{\sigma_0}^2}   \\
                \end{bmatrix}\right] }
    \end{split}
\end{equation}
We know from equation \ref{eq:deduce} that we need $\mu$ outside $\exp$ to construct $g(\boldsymbol{\eta})$.

If we create two helper parameters $\alpha,\beta$ to help us split the coefficients of $\mu$ and $\mu^2$, we'll have
\begin{equation}
    \begin{split}
        \frac{1}{\sqrt{2 \pi} \sigma_{0}}\exp{\left(\frac{-{\mu_0}^2}{2{\sigma_0}^2}\right)}\exp{\left[\boldsymbol{\eta}^T\begin{bmatrix}
                    \frac{\mu_0}{{\sigma_0}^2} \\
                    -\frac{1}{2{\sigma_0}^2}   \\
                \end{bmatrix}\right]}
        =\frac{1}{\sqrt{2 \pi} \sigma_{0}}\exp{\left[\frac{\alpha}{{\sigma_0}^2}{\mu}^2+\frac{\beta\mu_0}{{\sigma_0}^2}\mu\right]\left\{-\frac{(1+2\alpha)\mu^2-2{\mu_0}(1-\beta)\mu+{\mu_0}^2}{2{\sigma_0}^2}\right\}}
    \end{split}
\end{equation}
We can assume that $\nu$ here is the common denominator of $\frac{\alpha}{{\sigma_0}^2}$ and $\frac{\beta\mu_0}{{\sigma_0}^2}$, which is $\frac{1}{{\sigma_0}^2}$. Under this assumption $$g(\boldsymbol{\eta})=\exp(\alpha{\boldsymbol{\eta}}_2+\beta\mu_0{\boldsymbol{\eta}}_1).$$
\ul{Unfortunately, there's so far no way for us to determine the values of $\alpha$ and $\beta$.}

\textbf{When it comes to the posterior}, we first have to tidy up the original expression and get
\begin{align*}
    \prod_{i=1}^N\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{1}{2}\left( \frac{x^{(i)}-\mu}{\sigma} \right)^2\right\}&=\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^N\exp\left\{ -\frac{1}{2\sigma^2}\left[ \sum_{i=1}^N\left( x^{(i)} \right)^2-2\mu\sum_{i=1}^N x^{(i)}+N\mu^2 \right]\right\}\\
    &=\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^N\exp\left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^N\left( x^{(i)} \right)^2\right\}\exp\left\{ \frac{\mu}{\sigma^2}\sum_{i=1}^N x^{(i)}-\frac{N}{2\sigma^2}\mu^2\right\}
\end{align*}
Recall our goal: $p(\mathbf{x}|\bm{\eta})=\left(\prod_{n=1}^{N}h(\mathbf{x}_n)\right)g(\bm{\eta})^{N}\exp\left\{\bm{\eta}^T\sum_{n=1}^{N}\mathbf{u}(\mathbf{x}_n)\right\}$, since from prior $\bm{\eta}=\begin{bmatrix}
    \mu\\\mu^2
\end{bmatrix}$, we have to separate a term that contains $N,\mu,\mu^2$ for $g(\bm{\eta})^N$. Again we wish to create some helper parameters $\alpha'$ and $\beta'$ such that the expression becomes
\begin{align*}
    \left( \frac{1}{\sqrt{2\pi}\sigma} \right)^N&\exp\left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^N\left( x^{(i)} \right)^2\right\}\exp\left\{ \alpha'N\mu\sum_{i=1}^N x^{(i)}+\beta'N\mu^2 \right\}\\
    &\exp\left\{ \left( \frac{\mu}{\sigma^2}\sum_{i=1}^N x^{(i)}-\alpha'N\mu\sum_{i=1}^N x^{(i)} \right)-\left( \beta'N\mu^2+\frac{N}{2\sigma^2}\mu^2 \right)\right\}\\
    =\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^N&\exp\left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^N\left( x^{(i)} \right)^2\right\}\cdot\left( \alpha'\mu\sum_{i=1}^N x^{(i)}+\beta'\mu^2 \right)^N\\
    &\exp\left\{ \mu\left( \frac{1}{\sigma^2}\sum_{i=1}^N x^{(i)}-\alpha'N\sum_{i=1}^N x^{(i)} \right)-\mu^2\left( \beta'N+\frac{N}{2\sigma^2} \right)\right\}.
\end{align*}

With one-to-one correspondence with the goal and we can find that 
\begin{align*}
    \prod_{n=1}^N h(\mathbf{x_n})&=\left( \frac{1}{\sqrt{2\pi}\sigma} \right)^N\exp\left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^N\left( x^{(i)} \right)^2\right\}\\
    g\left( \begin{bmatrix}
        \mu\\~\\\mu^2
    \end{bmatrix} \right)&=\begin{bmatrix}
        \mu\\~\\\mu^2
    \end{bmatrix}\cdot\begin{bmatrix}
        \alpha'\sum_{i=1}^N x^{(i)}\\~\\\beta'
    \end{bmatrix}\\
    \sum_{n=1}^N\mathbf{u}(\mathbf{x_n})&=\begin{bmatrix}
        \left( \frac{1}{\sigma^2}-\alpha'N \right)\sum_{i=1}^N x^{(i)}\\
        ~\\
        \beta'N+\frac{N}{2\sigma^2}
    \end{bmatrix}.
\end{align*}
\ul{However there is still no way to detemine the exact values of $\alpha'$ and $\beta'$.}

\newpage
\section{Scoring Rules}
Scoring rules is the simplest form of prediction mechanism. For every agent with some information, a scoring rule evaluates how close the information is from the actual outcome, and pays the agent in return for his/her information.

\subsection{Motivations}
Taking an event $\mathbf{X}$ with outcome space $\mathcal{X}$, we want to know something about each agent's belief $p(\mathbf{x})$ on the actual outcome $\mathbf{x}\in \mathcal{X}$, compare it with the actual outcome, and see how accurate the agent's prediction is. However, it is impossible to ask agent for his/her entire $p(\mathbf{x})$ probability distribution. What should we do?

We've already known that sufficient statistic is a very suitable parameter to characterize a probability distribution. Therefore, we can use it to represent agent's belief. We just ask each agent for the sufficient statistic $\mathbf{u}(\mathbf{x})$ as a representation of his/her belief $p(\mathbf{x})$. Then we are able to measure how accurate the agent can predict with a scoring rule based on the actual outcome.

Assume $\bm{\mu}=\mathbb{E}_p[\mathbf{u}(\mathbf{x})]$ is the \textit{expected} sufficient statistic over some $p(\mathbf{x})$ that takes $\mathbf{u}(\mathbf{x})$ as its sufficient statistic and is taken as an estimate of the agent's belief. Assume $\bm{\hat{\mu}}$ is the agent's report as an estimate of $\bm{\mu}$. Then, with the actual outcome denoted $\mathbf{x}$, the scoring rule takes the form:
\[S(\bm{\hat{\mu}},\mathbf{x}).\]
We can see that the scoring rule is a measure of how close the agent's belief is from the actual outcome.

\subsection{Incentive Compatibility}
\textbf{Incentive compatibility} is a property of prediction mechanism with which the best strategy for an agent to earn the most profit is to \textit{honestly} report all the information as soon as he/she has it. As a prediction mechanism, a proper scoring rule should leverage \textbf{incentive compatibility} in order to get real information from agents.

Assume that we already set the sufficient statistic to be $\mathbf{u}(\mathbf{x})$. Then for each $p\in \mathcal{P}$ that takes this $\mathbf{u}(\mathbf{x})$ as its sufficient statistic, we can calculate its \textit{expected} sufficient statistic $\bm{\mu}=\mathbb{E}_p[\mathbf{u}(\mathbf{x})]$. A scoring rule is thus called \textbf{proper} if it satisfies the following, for all such $p$, for all $\bm{\hat{\mu}}\neq\bm{\mu}$:
\[\mathbb{E}_p[S(\bm{\mu},\mathbf{x})]\geqslant \mathbb{E}_p[S(\bm{\hat{\mu}},\mathbf{x})].\]
Any scoring rule with this property actually encourages agents to report a probability distribution as close to the actual probability distribution of $\mathbf{X}$ as possible, which is in accordance with the essence of incentive compatibility.

\subsection{Logarithmic Scoring Rule}
Suppose that we have set a form of $\mathbf{u}(\mathbf{x})$. For some $p$ that takes $\mathbf{u}(\mathbf{x})$ as its sufficient statistic, a classic logarithmic scoring rule takes the form:
\[S(\bm{\mu},\mathbf{x})=\ln p(\mathbf{x};\bm{\mu}),\]
where $\bm{\mu}=\mathbb{E}_p[\mathbf{u}(\mathbf{x})]$ is the expected sufficient statistic over $p(\mathbf{x};\bm{\mu})$. \textbf{For the following sections, we will be only talking about logarithmic scoring rules.}

\subsection{Maximum Entropy Optimization}
Now that we have the general expression of logarithmic scoring rules, how can we determine which $p(\mathbf{x};\bm{\mu})$ to choose as an estimate of agents' beliefs, given a specific $\mathbf{u}(\mathbf{x})$?

For some particular form of $\mathbf{u}(\mathbf{x})$, denote the set $\mathcal{P}$ which contains all $p$ that takes $\mathbf{u}(\mathbf{x})$ as its sufficient statistic. Then, we formulate the following optimization problem:
\begin{align*}
    \min_{p\in \mathcal{P}} & \quad F(p)=\int_{\mathbf{x}\in \mathcal{X}}p(\mathbf{x})\ln p(\mathbf{x})\ \mathrm{d}h(\mathbf{x}) \\
    \mathrm{s.t.}           & \quad \mathbb{E}_p[\mathbf{u}(\mathbf{x})]=\bm{\mu}
\end{align*}
where $\bm{\mu}$ is the expected sufficient statistic over $p$, and $h(\mathbf{x})$ is the base measure.

It is clear to see that $F(p)$ is actually the negative of entropy of $p(\mathbf{x})$. Therefore, minimizing $F(p)$ is thus maximizing the entropy of $p$. This actually mean that of all $p\in \mathcal{P}$, we tend to choose a $p$ that shows the most uncertainty of $\mathbf{X}$ as an estimate of agents' beliefs. This estimate is not necessarily exactly the same as agents' beliefs, as the general goal is just to find something that we can use to construct a metric in order to measure how close the agents' beliefs are from the actual outcome.

It has been proved that $S(\bm{\mu},\mathbf{x})=\ln p(\mathbf{x};\bm{\mu})$ is \textbf{proper} if and only if $p$ is the exponential family. It has also been proved that the solutions to the optimization problem are exponential family distributions which takes the form
\begin{align*}
    p(\mathbf{x};\bm{\eta}) & =h(\mathbf{x})g(\bm{\eta})\exp\left\{ \bm{\eta}\cdot\mathbf{u}(\mathbf{x})\right\}  \\
                            & =h(\mathbf{x})\exp\left\{ \bm{\eta}\cdot\mathbf{u}(\mathbf{x})-T(\bm{\eta})\right\}
\end{align*}
where $h(\mathbf{x})$ is the base measure, $\bm{\eta}$ is the natural parameter of $p$, $T(\bm{\eta})=-\ln g(\bm{\eta})$ is the log-partition function of $p$.

Either \textit{expected} sufficient statistic $\bm{\mu}$ or natural parameter $\bm{\eta}$ can parametrize a probability distribution $p$, and actually they are closely related. If we denote the convex conjugate of $T(\bm{\eta})$ as $G(\bm{\mu})$, then we have $\bm{\mu}=\nabla T(\bm{\eta}),\bm{\eta}=\nabla G(\bm{\mu})$.
\subsubsection{Examples}
\begin{enumerate}
    \item If we take the outcome space $\mathcal{X}=[0,+\infty)$ and $u(x)=x$, the maximum entropy optimization produces an exponential distribution. Hence, the log scoring rule becomes
          \[S(\mu,x)=-\frac{x}{\mu}-\ln\mu.\]

          If we take the partial derivative with respect to $\mu$
          \[\frac{\partial S}{\partial\mu}=\frac{x}{\mu^2}-\frac{1}{\mu}=\frac{x-\mu}{\mu^2},\]
          we can see that the scoring rule reaches maximum when $\mu=x$. This actually means that agents should report information as close to the actual outcome as possible in order to maximize profit, which corresponds to the essence of incentive compatibility.
    \item If we take the outcome space $\mathcal{X}=\mathbb{R}$ and $\mathbf{u}(x)=(x,x^2)$, the maximum entropy optimization produces a Gaussian distribution. Hence, the log scoring rule becomes
          \[S((\mu,\sigma^2),x)=-\frac{(x-\mu)^2}{\sigma^2}-\ln\sigma^2.\]

          As this scoring rule is a basic parabola with respect to $\mu$, it is also clear that it reaches the maximum when $\mu=x$. Similarly, this scoring rule is also incentive compatible.
\end{enumerate}

\section{Cost Function based Prediction Market with Bayesian Traders}
Based on what we have in previous sections, we could simulate the prediction market by a Python program.

For the setup, there would be a group of Bayesian agents (the number of agents as hyper-parameter would be an input for the program), a market maker and an executable program driving the market. They are explained in Sec~\ref{Bayesian-agents}, Sec~\ref{trading-precedure} and Sec~\ref{main-executable}. The dataset the market is based on is subject to Bernoulli distribution.

\subsection{Bayesian Agents}\label{Bayesian-agents}
Each Bayesian agents would be initialized with several data points, based on which he would form his prior $p(\boldsymbol{\eta}; \boldsymbol{\chi}, \nu)$. The dataset is subject to Bernoulli distribution, %based on  equation in Helen's part
 in which $\nu$ would be the number of trades the market maker publishes, $\boldsymbol{\chi}$ would be the current market price.

Suppose each agent would have observed $m$ data points before his or her entrance to the market. He would update his belief by % equation in Helen's part 
$$\nu\leftarrow\nu+m$$ $$\nu\bm{\chi} \leftarrow \sum_{n=1}^{N} \mathbf{u}\left(\mathbf{x}_{n}\right)+\nu \bm{\chi}$$

Since we are using Bernoulli distribution, the $\mathbf{x}_n$ in question is just a scalar that can take the value of either 0 or 1. The sufficient statistics for Bernoulli distribution is $x$. In other words,
\begin{displaymath}
\mathbf{u}(x) = x
\end{displaymath}
Since there is only one entry for the sufficient statistics, parameter $\boldsymbol{\eta}$ would be a scalar, which can be denoted as $\eta$. 
The agent can also derive the amount of outstanding shares $\eta$ from the current market price $\xi$, in our case, 
\begin{equation}
    \xi(\eta) = \nabla_{\eta}\exp{(g(\eta))}=\nabla_{\eta}\ln \left(1+e^{\eta}\right)=\frac{e^{\eta}}{1+e^{\eta}}
\end{equation}
Based on % equation Helen's part.
$$\xi(\eta+\delta)=\frac{\nu m \chi+\sum_{n=1}^{m}x_n}{\nu m+m}$$ the agent would be able to decide $\delta$, the amount he would like to sell is
$$

    \delta=\ln{(\frac{\nu m \chi+\sum_{n=1}^{m}x_n}{\nu m+m-(\nu m \chi+\sum_{n=1}^{m}x_n)})}-\eta
$$

% \subsection{Market Maker}\label{market-maker}
% The market maker is initialized with the initial outstanding security amount $\theta$ and the initial market price, which are hyper-parameter predefined.

% The market maker holds the sufficient statistic $\phi(x) = x$, cost function for the market $C(\theta) = - \log (- \theta)$ and price function $p = - 1 / \theta$. He would keep track of number of trades, outstanding security amount and current market price when the market runs.


\subsection{Executable Program}\label{main-executable}
The driving program takes agent number and max iteration number as hyper-parameters. The complete dataset is divided into two parts as random chosen data in the first part would be assigned to agents to form their priors, data in the second part would be fed to agents when they intend to enter the market.

Each agent comes into the market iteratively with the total number of iteration not exceeding max iteration number in program arguments. The interval between each agent's arrival is subject to Poisson distribution.  % The Poisson distribution part might be dismissed since the data is not time serializable. 
As discussed in Sec~\ref{Bayesian-agents} and Sec~\ref{trading-precedure}, each agent would update their belief and then trade with market maker. The final market price is considered as the aggregated belief among agents.


\end{document}